{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import os, os.path\n",
    "from sys import stdout\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# returns python object representation of JSON in response\n",
    "def get_response(symbol, older_than, retries=5):\n",
    "    url = 'https://api.stocktwits.com/api/2/streams/symbol/%s.json?max=%d' % (symbol, older_than-1)\n",
    "    for _ in range(retries):\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            return json.loads(response.content)\n",
    "        elif response.status_code == 429:\n",
    "            print response.content\n",
    "            return None\n",
    "        time.sleep(1.0)\n",
    "    # couldn't get response\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extends the current dataset for a given symbol with more tweets\n",
    "def get_older_tweets(symbol, num_queries):    \n",
    "    path = './data/%s.json' % symbol\n",
    "    if os.path.exists(path):\n",
    "        # extending an existing json file\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            if len(data) > 0:\n",
    "                older_than = data[-1]['id']\n",
    "            else:\n",
    "                older_than = 1000000000000\n",
    "    else:\n",
    "        # creating a new json file\n",
    "        data = []\n",
    "        older_than = 1000000000000 # any huge number\n",
    "    \n",
    "    for i in range(num_queries):\n",
    "        content = get_response(symbol, older_than)\n",
    "        if content == None:\n",
    "            print 'Error, an API query timed out'\n",
    "            break\n",
    "        data.extend(content['messages'])\n",
    "        older_than = data[-1]['id']\n",
    "        stdout.write('\\rSuccessfully made query %d' % (i+1))\n",
    "        stdout.flush()\n",
    "        # sleep to make sure we don't get throttled\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "    # write the new data to the JSON file\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "    print\n",
    "    print 'Done'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tweets for symbol JNUG\n",
      "{\"response\":{\"status\":429},\"errors\":[{\"message\":\"Rate limit exceeded. Client may not make more than 200 requests an hour.\"}]}\n",
      "Error, an API query timed out\n",
      "\n",
      "Done\n",
      "Getting tweets for symbol LABU\n",
      "{\"response\":{\"status\":429},\"errors\":[{\"message\":\"Rate limit exceeded. Client may not make more than 200 requests an hour.\"}]}\n",
      "Error, an API query timed out\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# get some data\n",
    "# apparently a client can only make 200 requests an hour, so we can't get all the data at once\n",
    "\n",
    "# make data directory if needed\n",
    "if not os.path.exists('./data'):\n",
    "    os.mkdir('./data')\n",
    "    \n",
    "symbols = ['AAPL', 'TSLA', 'NVDA', 'AMD', 'JNUG', 'LABU']\n",
    "tweets_per_symbol = 3000\n",
    "for symbol in symbols:\n",
    "    path = './data/%s.json' % symbol\n",
    "    if os.path.exists(path):\n",
    "        with open(path, 'r') as f:\n",
    "            num_tweets = len(json.load(f))\n",
    "    else:\n",
    "        num_tweets = 0\n",
    "    num_queries = (tweets_per_symbol - num_tweets - 1)/30 + 1\n",
    "    if num_queries > 0:\n",
    "        print 'Getting tweets for symbol %s' % symbol\n",
    "        get_older_tweets(symbol, num_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed\n"
     ]
    }
   ],
   "source": [
    "# check that we're doing the querying and appending correctly without getting duplicates\n",
    "# and that message IDs are in descending order\n",
    "symbol = 'TSLA'\n",
    "with open('./data/%s.json' % symbol, 'r') as f:\n",
    "    data = json.load(f)\n",
    "S = set()\n",
    "old_id = 1000000000000\n",
    "for message in data:\n",
    "    message_id = message['id']\n",
    "    assert message_id not in S\n",
    "    assert message_id < old_id\n",
    "    old_id = message_id\n",
    "    S.add(message_id)\n",
    "print 'Passed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tweets_and_labels(data):\n",
    "    # filter out messages without a bullish/bearish tag\n",
    "    data = filter(lambda m: m['entities']['sentiment'] != None, data)\n",
    "    # get tweets\n",
    "    tweets = map(lambda m: m['body'], data)\n",
    "    # get labels\n",
    "    def create_label(message):\n",
    "        sentiment = message['entities']['sentiment']['basic']\n",
    "        if sentiment == 'Bearish':\n",
    "            return 0\n",
    "        elif sentiment == 'Bullish':\n",
    "            return 1\n",
    "        else:\n",
    "            raise Exception('Got unexpected sentiment')\n",
    "    labels = map(create_label, data)\n",
    "    return tweets, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4227 labeled examples extracted \n"
     ]
    }
   ],
   "source": [
    "# get all tweets and labels available\n",
    "tweets = []\n",
    "labels = []\n",
    "all_tweets = []\n",
    "for filename in os.listdir('./data'):\n",
    "    path = './data/%s' % filename\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    all_tweets.extend(map(lambda m: m['body'], data))\n",
    "    t, l = get_tweets_and_labels(data)\n",
    "    tweets.extend(t)\n",
    "    labels.extend(l)\n",
    "assert len(tweets) == len(labels)\n",
    "print '%d labeled examples extracted ' % len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfidf_matrix(tweets, all_tweets=None):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    if all_tweets != None:\n",
    "        # use all tweets, including unlabeled, to learn vocab and tfidf weights\n",
    "        vectorizer.fit(all_tweets)\n",
    "    else:\n",
    "        vectorizer.fit(tweets)\n",
    "    return vectorizer.transform(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_svm(X, y):\n",
    "    model = svm.LinearSVC(penalty='l2', loss='hinge', C=1.0)\n",
    "    #model = svm.SVC(C=1.0, kernel='rbf')\n",
    "    model.fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4227, 11691)\n",
      "(4227,)\n",
      "Training set size is 3804\n",
      "Percent bullish = 76.271587%\n"
     ]
    }
   ],
   "source": [
    "X = tfidf_matrix(tweets, all_tweets)\n",
    "y = np.array(labels)\n",
    "print X.shape\n",
    "print y.shape\n",
    "\n",
    "N = X.shape[0]\n",
    "num_train = int(math.floor(N*0.9))\n",
    "P = np.random.permutation(N)\n",
    "X_tr = X[P[:num_train]]\n",
    "y_tr = y[P[:num_train]]\n",
    "X_te = X[P[num_train:]]\n",
    "y_te = y[P[num_train:]]\n",
    "print 'Training set size is %d' % num_train\n",
    "print 'Percent bullish = %f%%' % (100*y.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy = 0.915615\n",
      "Test set accuracy = 0.825059\n"
     ]
    }
   ],
   "source": [
    "model = train_svm(X_tr, y_tr)\n",
    "print 'Training set accuracy = %f' % model.score(X_tr, y_tr)\n",
    "print 'Test set accuracy = %f' % model.score(X_te, y_te)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
